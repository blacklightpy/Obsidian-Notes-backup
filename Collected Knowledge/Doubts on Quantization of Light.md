- If light travels for long distances, will it's intensity be less than a quantum?
- Space is expanding - cosmic redshift - frequency decreases over distances. Does this have anything to do with it?

# Idea 2
Perhaps it is not the frequency, but the [[Impulse]] of the electromagnetic force that causes the electron to fall out of its energy level?

Edit: no, it may be the impulse over time, which is in other words, force, which moves causes the expulsion.
# Idea 1
Think of light flowing spherically from a source. The intensity of light is higher the closer you are to the source, for a given size of region. There is no discrete count, but instead the solid angle of the region of absorption with respect to the source determines the total energy.

That is to say, in my intuition, $E=nh\nu$ is incorrect, and instead it should be $E=\Omega h\nu$, where $\Omega$ is the [[Solid angle]]. In natural units, we can omit the $h$ and say $E=\nu$ for the minimum energy of a packet. Now of course, I'm going against the conventional explanation, but I'm not baseless in that. I was simply not satisfied with the interpretation taken based on the observations, as outlined in the NCERT chemistry textbook. I can relate this to photometric quantities like [[Luminous Intensity]] or [[Luminance]], and also with the representation of EM wave propagation using the 'Field of a Moving Charge' applet.

Considering the concept of solid angles, this means energy is not defined for any arbitrary region, since the solid angle can be very small for a given region. Makes me wonder if $h$ is defined with a specific area in mind, perhaps it is the Planck area I guess.

So, to answer my question, if the lowest area is defined as the Planck area, I guess, the intensity of light can take a minimum value, but still since the angle is dependent on area and the distance, I think a minimum value only exists at a given distance from a source. This is a nice theory which I'd like to experiment on.